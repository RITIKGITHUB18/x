Machine Learning Assignment

                                   Assignment-01

import pandas as pd
import numpy as np

df = pd.read_csv('Heart.csv')
df

missing_values = df.isna().sum()
print('missing values: ',missing_values)

data_shapes=df.shape
print("Data Shape: ",data_shapes)

dataType = df.dtypes
print("Data Types of Each Attributes: ",dataType)

zero_counts=(df==0).sum()
print("Numbers of Zeroes in Each Column: \n",zero_counts)

mean_age=df['Age'].mean()
print('Mean age of patients: ',mean_age)

x=df[['Age','Chol']].max()
x

df['AHD'].replace('Yes',1,inplace=True)
df['AHD'].replace('No',0,inplace=True)
df

x=df.drop(columns=['SNo','AHD'])
x

from sklearn.model_selection import train_test_split
xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=1)
xTrain,xTest,yTrain,yTest

cor=df.corr(numeric_only=True)['AHD'].sort_values(ascending=False)
cor

import matplotlib.pyplot as plt
y = df['Age']
x = df['Chol'] 
colors = ['red' if value == 1 else 'blue' for value in df['AHD']]

plt.scatter(x[df['AHD'] == 0], y[df['AHD'] == 0], c='blue', marker='o', label='No')
plt.scatter(x[df['AHD'] == 1], y[df['AHD'] == 1], c='red', marker='o', label='Yes')
plt.title('Age vs. Chol')
plt.ylabel('Age')
plt.xlabel('Chol')
legend = plt.legend()
plt.show()






                                        Assignment-02


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from sklearn.model_selection import train_test_split

df = pd.read_csv("temperatures.csv")
df

df.shape

# input features
x= df["ANNUAL"].values

p=df.corr()["ANNUAL"].sort_values(ascending=False)
p

# Highly correlated
y = df["DEC"].values


# 2D array
x=x.reshape(117,1)
y=y.reshape(117,1)
x,y

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state = 1)

# instacnce is created
model_highly = LinearRegression()
model_highly.fit(x_train,y_train)

y_pred =  model_highly.predict(x_test)
y_pred_training = model_highly.predict(x_train)

#actual dp
plt.scatter(x_test,y_test,color='b') 
plt.plot(x_test,y_pred,color='orange',linewidth = 1)
plt.title("ANNUAL vs DEC")
plt.ylabel("DEC")
plt.xlabel("ANNAUL Temp")
plt.show()

# Testing error
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test , y_pred)
print(f'MSE:{mse}')
print(f'MAE:{mae}')
print(f'R-squared (R2): {r2}')

# training error
mse = mean_squared_error(y_train,y_pred_training)
mae = mean_absolute_error(y_train,y_pred_training)
print(f'MSE: {mse}')
print(f'MAE: {mae}')
print(f'R-squared (R2): {r2}')

#Least correlated
y=df["JUN"].values
y=y.reshape(117,1)
y

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=1)


model_least = LinearRegression()
model_least.fit(x_train,y_train)


y_pred = model_least.predict(x_test)
y_pred_training = model_least.predict(x_train)

plt.scatter(x_test,y_test,color='b')
plt.plot(x_test,y_pred,color='orange',linewidth = 1)
plt.title("ANNUAL vs JUN")
plt.ylabel("JUN")
plt.xlabel("ANNUAL TEMP")
plt.show()

#testing error
mse = mean_squared_error(y_test,y_pred)
mae = mean_absolute_error(y_test,y_pred)
r2 = r2_score(y_test,y_pred)
print(f'MSE: {mse}')
print(f'MAE: {mae}')
print(f'R-squared (R2): {r2}')


#testing error
mse = mean_squared_error(y_test,y_pred)
mae = mean_absolute_error(y_test,y_pred)
r2 = r2_score(y_test,y_pred)
print(f'MSE: {mse}')
print(f'MAE: {mae}')
print(f'R-squared (R2): {r2}')

training_percentages=[]
training_errors=[]
testing_errors=[]
for i in range (1,10):
    percentage = i/10.0
    training_percentages.append(percentage)

    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=1-percentage,random_state = 0)
    model = LinearRegression()
    model.fit(X_train,y_train)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    training_sse = np.sum((y_train-y_train_pred)**2)
    testing_sse = np.sum((y_test-y_test_pred)**2)

    training_errors.append(training_sse)
    testing_errors.append(testing_sse)

#plotting 
    
plt.plot(training_percentages, training_errors, label="Training Error (SSE)")
plt.plot(training_percentages, testing_errors, label="Testing Error (SSE)")
plt.xlabel("Training Percentage")
plt.ylabel("SSE")
plt.legend()
plt.title("Training and Testing Error vs. Training Percentage")
plt.show()







                                           Assignment-03


import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder


data= pd.read_csv("Admission_Predict.csv")
data

# Convert the "Admitted" column to binary (0 or 1)
data["Chance of Admit "] = data["Chance of Admit "].apply(lambda x: 1 if x > 0.8 else 0)
data

x=data.drop("Chance of Admit ",axis=1)# in panda axis 1 represent column and axis 0 represent rows
y=data["Chance of Admit "]
x

y=y.astype("int")

sb.countplot(x=y)

y.value_counts()

x_train.shape #shape determine the dimension of dataframes

classifier = DecisionTreeClassifier(random_state=0,criterion='entropy')
classifier.fit(x_train, y_train)

#For Training
classifier1 = DecisionTreeClassifier(random_state=0)
classifier1.fit(x_train, y_train)


# Evaluate the Model
y_pred = classifier.predict(x_test)
y_pred1 = classifier1.predict(x_test)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

result=pd.DataFrame({
    'actual':y_test,
    'predicted':y_pred
})
print(result)


ConfusionMatrixDisplay.from_predictions(y_test,y_pred)


# Display classification report and confusion matrix
print("Classification Report for entropy:")
print(classification_report(y_test, y_pred)
print("Classification Report for gini index:")
print(classification_report(y_test, y_pred1))


# Plot the Decision Tree for testing dataset
plt.figure(figsize=(25, 15))
plot_tree(classifier,fontsize=10,filled=True,rounded=True)
plt.title("Decision Tree with entropy")
plt.show()

print("")

plt.figure(figsize=(25, 15))
plot_tree(classifier1,fontsize=10,filled=True,rounded=True)
plt.title("Decision Tree with gini index")
plt.show()

#For training dataset

classifier = DecisionTreeClassifier(random_state=0,criterion='entropy')
classifier.fit(x_train, y_train)

classifier1 = DecisionTreeClassifier(random_state=0)
classifier1.fit(x_train, y_train)

# Evaluate the Model
y_pred = classifier.predict(x_train)
y_pred1 = classifier1.predict(x_train)


print("Confusion Matrix:")
print(confusion_matrix(y_train, y_pred))

result=pd.DataFrame({
    'actual':y_train,
    'predicted':y_pred
})
result

ConfusionMatrixDisplay.from_predictions(y_train,y_pred)

# Display classification report and confusion matrix
print("Classification Report for entropy :")
print(classification_report(y_train, y_pred))
print("")
print("Classification Report for gini index:")
print(classification_report(y_train, y_pred1))

# Plot the Decision Tree
plt.figure(figsize=(25, 15))
plot_tree(classifier,fontsize=10,filled=True,rounded=True)
plt.title("Decision Tree with entropy index")
plt.show()
print("")
plt.figure(figsize=(25, 15))
plot_tree(classifier1,fontsize=10,filled=True,rounded=True)
plt.title("Decision Tree with gini  index")
plt.show()






                                         Assignment-04



import pandas as pd
import numpy as np
import seaborn as sb
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Load the dataset
data = pd.read_csv("mall_customers.csv")
print(data)

x=data.iloc[:,3:]

plt.title("Unclustered Data")
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.scatter(x["Annual Income (k$)"],x["Spending Score (1-100)"])


km=KMeans(n_clusters=3,n_init = 'auto')
km.fit_predict(x)

#Sum Squared error number of clusters increase SSE value decreases
km.inertia_


sse=[]
for k in range(1,16):
    km=KMeans(n_clusters=k)
    km.fit_predict(x)
    sse.append(km.inertia_)


plt.title("Elbow Method")
plt.xlabel("Value of x")
plt.ylabel("SSE")
plt.plot(range(1,16),sse,marker='.',color='green',label="X Values")


silh=[]
for k in range(2,16):
    km=KMeans(n_clusters=k)
    labels=km.fit_predict(x)
    score=silhouette_score(x,labels)
    silh.append(score)
silh


plt.title("Silhoutte Method")
plt.xlabel("Value of x")
plt.ylabel("Silhoutte Score")
plt.plot(range(2,16),silh,marker='.',color="green")
plt.show()


km=KMeans(n_clusters=5,n_init='auto')


labels=km.fit_predict(x)
labels


plt.title("Unclustered Data")
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.scatter(x["Annual Income (k$)"],x["Spending Score (1-100)"])


plt.title("Clustered Data")
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.scatter(x["Annual Income (k$)"],x["Spending Score (1-100)"],c=labels)


data[labels==4]


km.predict([[46,78]])


clusters = km.cluster_centers_
clusters


# AgglomerativeClustering
agl = AgglomerativeClustering(n_clusters=5)
alabels=agl.fit_predict(x)


plt.title("Agglomerative")
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.scatter(x["Annual Income (k$)"],x["Spending Score (1-100)"],c=alabels)


plt.title("KMeans")
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.scatter(x["Annual Income (k$)"],x["Spending Score (1-100)"],c=labels)
sb.scatterplot(x=clusters[:,0],y=clusters[:,1],color='red')
plt.show()

# Standardize the features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[['Annual Income (k$)','Spending Score (1-100)']])

# Train-Test Split
X_train,X_test = train_test_split(scaled_data,test_size=0.2,random_state=42)

# Perform hierarchical clustering
linkage_matrix = linkage(X_train,method = 'ward',metric = 'euclidean')

# plot the dendogram
plt.figure(figsize=(12,6))
dendrogram(linkage_matrix)
plt.title("Dendogram for Aggometric Hierarchical Clustering")
plt.xlabel('Sample')
plt.ylabel('Distance')
plt.show()


				Assignment - 05

import pandas as pd
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import matplotlib.pyplot as plt

data = pd.read_csv("Market_Basket_Optimisation.csv", header=None)
data

transactions = []
for i in range(len(data)):
    transaction = []
    for j in range(len(data.columns)):
        if str(data.values[i, j])!='nan':
            transaction.append(str(data.values[i, j]))
    transactions.append(transaction)
transactions
            

from mlxtend.preprocessing import TransactionEncoder
te = TransactionEncoder()
x = te.fit_transform(transactions)

len(te.columns_)

te.columns_

df = pd.DataFrame(x, columns=te.columns_)
df


freq_itemset = apriori(df, min_support=0.01, use_colnames=True)
freq_itemset

rules = association_rules(freq_itemset, metric='confidence', min_threshold=0.10)

print("List of Transactions:")
for i, transaction in enumerate(transactions[:5]):  # Print the first 5 transactions
    print(f"Transaction {i + 1}: {transaction}")

rules.head(100)

plt.scatter(rules['support'], rules['confidence'], alpha=0.5)
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Support vs. Confidence')
plt.show()

rules[rules['antecedents'] == {'cake'}]


# Network graph 
import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()

# Add nodes and edges for association rules
for i, rule in rules.iterrows():
    antecedents = ", ".join(rule['antecedents'])
    consequents = ", ".join(rule['consequents'])
    rule_name = f"{antecedents} -> {consequents}"
    confidence = rule['confidence']

    G.add_node(antecedents, label=antecedents)
    G.add_node(consequents, label=consequents)
    G.add_edge(antecedents, consequents, label=f"Confidence: {confidence:.3f}")

# Create a mapping of node labels
labels = {node: G.nodes[node]['label'] for node in G.nodes}
edge_labels = {edge: G.edges[edge]['label'] for edge in G.edges}

plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G, k=15)

nx.draw(G, pos, labels=labels, node_size=2000, font_size=8, node_color='lightblue', with_labels=True)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
plt.axis('off')
plt.show()


				Assignment-06

import pandas as pd
from keras.models import Sequential
from keres.layers import Dense
from ann_visualizer_visualize import ann_viz

df=pd.read_csv('pima-indians-diabetes.csv')

df.info

df.columns

X=df.iloc[:,0:-1].values

y=df.iloc[:,8].values

model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# fitting of model
model.fit(X, y, epochs=200, batch_size=10)

# evaluation
_, accuracy = model.evaluate(X, y)
print('Accuracy: %.2f' % (accuracy*100))


ann_viz(model,view=True, file="Neural_Network.pdf", title = 'MyNeural Network')



